# llm-eval

Personal LLM evaluation harness. Build up your own benchmarks over time instead of trusting public leaderboards.

Each model gets a persistent JSON file in `results/`. When a new model drops, run the eval, score it, and compare against everything you've tested before.

## Quick Start

```bash
pip install -r requirements.txt

cp config.example.yaml config.yaml
# Edit config.yaml — add your API keys

export ANTHROPIC_API_KEY=sk-...
export OPENAI_API_KEY=sk-...

# Run eval against a model
python run.py eval claude-sonnet-4

# Score the results
python run.py score claude-sonnet-4

# Later, when a new model comes out:
python run.py eval gpt-5
python run.py score gpt-5

# Compare everything
python run.py compare
```

## Workflow

```
┌─────────────────────────────────────────────────────┐
│  New model drops                                    │
│                                                     │
│  1. Add to config.yaml                              │
│  2. python run.py eval <model>     → auto-checks    │
│  3. python run.py score <model>    → human 1-5      │
│  4. python run.py compare          → leaderboard    │
│                                                     │
│  results/<model>.json accumulates over time         │
└─────────────────────────────────────────────────────┘
```

## Commands

```bash
# ── Evaluate ──
python run.py eval claude-sonnet-4                  # All prompts
python run.py eval claude-sonnet-4 --ids C01 L02    # Specific prompts
python run.py eval claude-sonnet-4 --category coding --difficulty hard
python run.py eval claude-sonnet-4 --rerun          # Re-run (appends, keeps history)

# ── Score ──
python run.py score claude-sonnet-4                 # Score all
python run.py score claude-sonnet-4 --unscored      # Only unscored prompts
python run.py score claude-sonnet-4 --ids C01       # Re-score specific

# ── Compare ──
python run.py compare                               # All models
python run.py compare claude-sonnet-4 gpt-4o        # Specific models
python run.py compare --category coding             # By category
python run.py compare --save                        # Save markdown report

# ── Browse ──
python run.py models                                # List evaluated models
python run.py prompts                               # List eval prompts
python run.py prompts --difficulty hard              # Filter prompts
```

## Results Structure

Each model gets its own file:

```
results/
├── claude-sonnet-4.json      # Persists across runs
├── gpt-4o.json
├── gemini-2-flash.json
├── llama-3-70b.json
└── comparison.md             # Generated by --save
```

Each model file stores run history per prompt:

```json
{
  "model_name": "claude-sonnet-4",
  "created": "2025-02-06T...",
  "updated": "2025-02-06T...",
  "runs": {
    "C01": [
      {
        "timestamp": "2025-02-06T...",
        "api_model": "claude-sonnet-4-20250514",
        "content": "...",
        "latency_s": 3.2,
        "input_tokens": 245,
        "output_tokens": 612,
        "auto_checks": { "flags": [], "passed": true },
        "human_score": 4,
        "human_notes": "Good but missed negative k edge case"
      }
    ]
  }
}
```

Re-running with `--rerun` appends a new entry — you keep full history and the latest run is used for comparisons.

## Auto-Checks

Every response gets automated checks that flag mechanical issues without replacing your judgment:

| Check | What it catches |
|---|---|
| `trap_no_bug` | Model invents a phantom bug in working code (C01) |
| `trap_common_error` | Model confuses memory vs compute complexity (L02) |
| `trap_wrong_claim` | Model agrees with your wrong claim instead of correcting (M01) |
| `json_valid` | Response isn't valid JSON when asked for JSON |
| `constraint_check` | Wrong count, included excluded terms |
| `refusal_check` | Unnecessary refusal on legitimate requests |
| `ambiguity_check` | Didn't ask for clarification on vague input |
| `word_count` | Over/under target word count |
| `self_awareness` | Doesn't acknowledge known limitations |

## Adding Models

Any OpenAI-compatible API works (vLLM, Ollama, Together, Groq, etc.):

```yaml
# In config.yaml
llama-3-70b:
  provider: openai_compatible
  model: meta-llama/Llama-3-70b
  api_key_env: none
  base_url: http://localhost:8000/v1
  params:
    max_tokens: 4096
    temperature: 0
```

## Adding Prompts

Edit `evals/default.json`. Each prompt:

```json
{
  "id": "X01",
  "category": "your_category",
  "subcategory": "specific_area",
  "difficulty": "easy|medium|hard",
  "prompt": "The actual prompt",
  "ideal": "What good looks like",
  "criteria": ["what", "you", "judge"],
  "check_type": "reasoning"
}
```

After adding prompts, run existing models with `--rerun` or just `eval` (only new prompts run by default).

## Project Structure

```
llm-eval/
├── run.py                  # CLI: eval, score, compare, models, prompts
├── config.example.yaml     # Template — copy to config.yaml
├── requirements.txt
├── evals/
│   └── default.json        # 24 eval prompts
├── scripts/
│   ├── providers.py        # Anthropic, OpenAI, Google, OpenAI-compatible
│   └── checks.py           # 13 automated response checkers
└── results/                # Per-model JSON files (git-ignored)
```
